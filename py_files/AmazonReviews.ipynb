{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T05:36:21.777558Z",
     "start_time": "2018-11-08T05:36:21.245383Z"
    }
   },
   "outputs": [],
   "source": [
    "import AmazonReviews\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T05:36:22.624001Z",
     "start_time": "2018-11-08T05:36:22.580848Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T06:18:07.169180Z",
     "start_time": "2018-11-08T06:17:10.097388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from pickle...\n"
     ]
    }
   ],
   "source": [
    "ar = AmazonReviews.AmazonReviews()\n",
    "\n",
    "PATH = '../data/amazon_reviews_us_Toys_v1_00.tsv'\n",
    "ar.load_data(PATH)\n",
    "\n",
    "ar.calc_trend_score(review_days=30)\n",
    "\n",
    "ar.create_observations()\n",
    "\n",
    "ar.create_train_test_split(train_reduction=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the distribution of the first review data. 1/1/2014 is the most popular. May need to move the cutoff date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014-01-01    0.030230\n",
    "2014-01-02    0.029510\n",
    "2014-01-03    0.026328\n",
    "2014-01-04    0.016026\n",
    "2014-01-07    0.014318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T05:37:06.684969Z",
     "start_time": "2018-11-08T05:37:06.585607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014-01-01    0.030230\n",
       "2014-01-02    0.029510\n",
       "2014-01-03    0.026328\n",
       "2014-01-04    0.016026\n",
       "2014-01-07    0.014318\n",
       "Name: min_review_date, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.reviews_selected_df.min_review_date.value_counts(normalize=True).sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T05:37:06.827748Z",
     "start_time": "2018-11-08T05:37:06.687748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>orig_std</th>\n",
       "      <th>review_success</th>\n",
       "      <th>trend_score</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5469.000000</td>\n",
       "      <td>5469.000000</td>\n",
       "      <td>5469.000000</td>\n",
       "      <td>5469.000000</td>\n",
       "      <td>5469.000000</td>\n",
       "      <td>5469.000000</td>\n",
       "      <td>5469.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>73.554032</td>\n",
       "      <td>10.885160</td>\n",
       "      <td>2.182931</td>\n",
       "      <td>2.114140</td>\n",
       "      <td>11.285563</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>81.650357</td>\n",
       "      <td>0.954892</td>\n",
       "      <td>1.027999</td>\n",
       "      <td>1.149140</td>\n",
       "      <td>9.822584</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>0.463900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.964506</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>1.386188</td>\n",
       "      <td>1.386188</td>\n",
       "      <td>6.869424</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>2.382850</td>\n",
       "      <td>2.382850</td>\n",
       "      <td>8.231919</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>2.968471</td>\n",
       "      <td>2.968471</td>\n",
       "      <td>11.643701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1268.000000</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>4.365866</td>\n",
       "      <td>4.365866</td>\n",
       "      <td>167.967345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count       median          std     orig_std  review_success  \\\n",
       "count  5469.000000  5469.000000  5469.000000  5469.000000     5469.000000   \n",
       "mean     73.554032    10.885160     2.182931     2.114140       11.285563   \n",
       "std      81.650357     0.954892     1.027999     1.149140        9.822584   \n",
       "min       8.000000     5.196152     0.463900     0.000000        5.964506   \n",
       "25%      30.000000    11.180340     1.386188     1.386188        6.869424   \n",
       "50%      55.000000    11.180340     2.382850     2.382850        8.231919   \n",
       "75%      89.000000    11.180340     2.968471     2.968471       11.643701   \n",
       "max    1268.000000    11.180340     4.365866     4.365866      167.967345   \n",
       "\n",
       "       trend_score   trend  \n",
       "count  5469.000000  5469.0  \n",
       "mean      0.999998     1.0  \n",
       "std       0.000003     0.0  \n",
       "min       0.999987     1.0  \n",
       "25%       0.999998     1.0  \n",
       "50%       1.000000     1.0  \n",
       "75%       1.000000     1.0  \n",
       "max       1.000000     1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.product_trend_df[ar.product_trend_df.trend == 1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM Creation\n",
    "\n",
    "Create DTM and only restrict features to English words in the `nltk.corpus.words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T06:18:15.343123Z",
     "start_time": "2018-11-08T06:18:15.168789Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words, stopwords\n",
    "from nltk import SnowballStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T06:18:18.084978Z",
     "start_time": "2018-11-08T06:18:17.798128Z"
    }
   },
   "outputs": [],
   "source": [
    "words_corpus = set(words.words())\n",
    "stop_words = set(stopwords.words())\n",
    "# analyzer = CountVectorizer().build_analyzer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# should move this a tokenizer\n",
    "\n",
    "def english_corpus(doc, tkpat=re.compile('\\\\b[a-z][a-z]+\\\\b')):\n",
    "    return [stemmer.stem(w) for w in tkpat.findall(doc)]\n",
    "\n",
    "\n",
    "# ar.add_dtm(CountVectorizer(stop_words='english', tokenizer=english_corpus, min_df=2),'count_1_gram')\n",
    "# # ar.add_dtm(CountVectorizer(stop_words='english', min_df=0.005),'count_1_gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T06:18:21.801310Z",
     "start_time": "2018-11-08T06:18:21.751341Z"
    }
   },
   "outputs": [],
   "source": [
    "first_pipe = imbPipeline([('cnt_v', CountVectorizer(stop_words='english', tokenizer=english_corpus, min_df=2)),\n",
    "                          ('lda', LatentDirichletAllocation(n_jobs=-1, learning_method='online', random_state=42)),\n",
    "                          ('sm', SMOTE(random_state=42)),\n",
    "                          ('ss', StandardScaler()),\n",
    "                          ('log_reg', LogisticRegression(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T06:23:40.859188Z",
     "start_time": "2018-11-08T06:23:40.811092Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lda__n_components': Integer(5, 20),\n",
    "    'lda__learning_decay': Real(0.5, 1),\n",
    "    'log_reg__C': Categorical([0.001,0.01,0.1,1,10,100])\n",
    "}\n",
    "\n",
    "grid = BayesSearchCV(first_pipe, params, n_jobs=4, n_iter=5, scoring='precision', refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-08T06:26:14.853Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    }
   ],
   "source": [
    "ar.run_model(grid, 'mvp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:35:55.417962Z",
     "start_time": "2018-11-08T00:35:55.348668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2169"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ar.models['count_1_gram']['model'].get_feature_names()\n",
    "\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:35:55.495508Z",
     "start_time": "2018-11-08T00:35:55.420647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zipper',\n",
       " 'zip',\n",
       " 'zero',\n",
       " 'yr',\n",
       " 'younger',\n",
       " 'young',\n",
       " 'yoga',\n",
       " 'yesterday',\n",
       " 'yes',\n",
       " 'yellow',\n",
       " 'year',\n",
       " 'yeah',\n",
       " 'yay',\n",
       " 'yard',\n",
       " 'xxxl',\n",
       " 'xxx',\n",
       " 'xxs',\n",
       " 'xxl',\n",
       " 'xs',\n",
       " 'xmas',\n",
       " 'xlarg',\n",
       " 'xl',\n",
       " 'wrote',\n",
       " 'wrong',\n",
       " 'write',\n",
       " 'wrist',\n",
       " 'wrinkl',\n",
       " 'wrap',\n",
       " 'wrangler',\n",
       " 'wow',\n",
       " 'wouldnt',\n",
       " 'wouldn',\n",
       " 'worth',\n",
       " 'worst',\n",
       " 'wors',\n",
       " 'worri',\n",
       " 'worn',\n",
       " 'world',\n",
       " 'workout',\n",
       " 'workmanship',\n",
       " 'worker',\n",
       " 'work',\n",
       " 'wore',\n",
       " 'word',\n",
       " 'wool',\n",
       " 'wont',\n",
       " 'wonder',\n",
       " 'won',\n",
       " 'women',\n",
       " 'woman',\n",
       " 'witch',\n",
       " 'wit',\n",
       " 'wish',\n",
       " 'wise',\n",
       " 'wire',\n",
       " 'wipe',\n",
       " 'winter',\n",
       " 'wing',\n",
       " 'wine',\n",
       " 'window',\n",
       " 'wind',\n",
       " 'win',\n",
       " 'wiggl',\n",
       " 'wig',\n",
       " 'wife',\n",
       " 'width',\n",
       " 'wider',\n",
       " 'wide',\n",
       " 'wick',\n",
       " 'white',\n",
       " 'whi',\n",
       " 'wherev',\n",
       " 'whenev',\n",
       " 'whatev',\n",
       " 'wetsuit',\n",
       " 'wet',\n",
       " 'weren',\n",
       " 'went',\n",
       " 'weld',\n",
       " 'welcom',\n",
       " 'weird',\n",
       " 'weight',\n",
       " 'weigh',\n",
       " 'weekend',\n",
       " 'week',\n",
       " 'wed',\n",
       " 'websit',\n",
       " 'web',\n",
       " 'weav',\n",
       " 'weather',\n",
       " 'wearabl',\n",
       " 'wear',\n",
       " 'weak',\n",
       " 'wayyyyy',\n",
       " 'wayyyi',\n",
       " 'wayyy',\n",
       " 'way',\n",
       " 'waterproof',\n",
       " 'water',\n",
       " 'watch',\n",
       " 'wast',\n",
       " 'wasnt',\n",
       " 'wasn',\n",
       " 'washer',\n",
       " 'wash',\n",
       " 'warn',\n",
       " 'warmth',\n",
       " 'warmest',\n",
       " 'warmer',\n",
       " 'warm',\n",
       " 'ware',\n",
       " 'wardrob',\n",
       " 'want',\n",
       " 'walmart',\n",
       " 'walli',\n",
       " 'wallet',\n",
       " 'wall',\n",
       " 'walk',\n",
       " 'wait',\n",
       " 'waistlin',\n",
       " 'waistband',\n",
       " 'waist',\n",
       " 'wacoal',\n",
       " 'waaay',\n",
       " 'vs',\n",
       " 'visual',\n",
       " 'visit',\n",
       " 'vision',\n",
       " 'visibl',\n",
       " 'virtual',\n",
       " 'violet',\n",
       " 'vintag',\n",
       " 'vietnam',\n",
       " 'vibrant',\n",
       " 'vest',\n",
       " 'versus',\n",
       " 'version',\n",
       " 'versatil',\n",
       " 'verri',\n",
       " 'veri',\n",
       " 'vendor',\n",
       " 'velvet',\n",
       " 'velcro',\n",
       " 'veil',\n",
       " 'vega',\n",
       " 've',\n",
       " 'various',\n",
       " 'varieti',\n",
       " 'valu',\n",
       " 'valentin',\n",
       " 'vacat',\n",
       " 'util',\n",
       " 'usual',\n",
       " 'usp',\n",
       " 'user',\n",
       " 'useless',\n",
       " 'use',\n",
       " 'usabl',\n",
       " 'usa',\n",
       " 'ur',\n",
       " 'upset',\n",
       " 'upper',\n",
       " 'updat',\n",
       " 'upcom',\n",
       " 'unwear',\n",
       " 'unusu',\n",
       " 'unravel',\n",
       " 'unpleas',\n",
       " 'unlik',\n",
       " 'unless',\n",
       " 'uniqu',\n",
       " 'uniform',\n",
       " 'unfortun',\n",
       " 'unfold',\n",
       " 'unflatt',\n",
       " 'unfinish',\n",
       " 'unexpect',\n",
       " 'uneven',\n",
       " 'undon',\n",
       " 'undi',\n",
       " 'underwir',\n",
       " 'underwear',\n",
       " 'understand',\n",
       " 'undershirt',\n",
       " 'underneath',\n",
       " 'undergar',\n",
       " 'uncomfort',\n",
       " 'unbeliev',\n",
       " 'unabl',\n",
       " 'ultim',\n",
       " 'uk',\n",
       " 'ugli',\n",
       " 'ugg',\n",
       " 'ucla',\n",
       " 'typic',\n",
       " 'type',\n",
       " 'twist',\n",
       " 'twice',\n",
       " 'tuxedo',\n",
       " 'tux',\n",
       " 'tush',\n",
       " 'turtleneck',\n",
       " 'turtl',\n",
       " 'turquois',\n",
       " 'turn',\n",
       " 'tunic',\n",
       " 'tummi',\n",
       " 'tug',\n",
       " 'tuck',\n",
       " 'tube',\n",
       " 'tshirt',\n",
       " 'trust',\n",
       " 'trunk',\n",
       " 'truli',\n",
       " 'true',\n",
       " 'trouser',\n",
       " 'troubl',\n",
       " 'tropic',\n",
       " 'trip',\n",
       " 'trim',\n",
       " 'trifold',\n",
       " 'trick',\n",
       " 'tri',\n",
       " 'trend',\n",
       " 'trench',\n",
       " 'treat',\n",
       " 'travel',\n",
       " 'trash',\n",
       " 'transport',\n",
       " 'transpar',\n",
       " 'transfer',\n",
       " 'transact',\n",
       " 'trainer',\n",
       " 'train',\n",
       " 'tradit',\n",
       " 'trade',\n",
       " 'track',\n",
       " 'town',\n",
       " 'towel',\n",
       " 'tough',\n",
       " 'touch',\n",
       " 'total',\n",
       " 'torso',\n",
       " 'tore',\n",
       " 'took',\n",
       " 'tone',\n",
       " 'ton',\n",
       " 'tomorrow',\n",
       " 'toler',\n",
       " 'told',\n",
       " 'togeth',\n",
       " 'toe',\n",
       " 'toddler',\n",
       " 'today',\n",
       " 'titl',\n",
       " 'tire',\n",
       " 'tini',\n",
       " 'time',\n",
       " 'till',\n",
       " 'tighter',\n",
       " 'tighten',\n",
       " 'tight',\n",
       " 'tiger',\n",
       " 'tie',\n",
       " 'thx',\n",
       " 'thumb',\n",
       " 'throw',\n",
       " 'thrill',\n",
       " 'threw',\n",
       " 'thread',\n",
       " 'thought',\n",
       " 'thorough',\n",
       " 'thong',\n",
       " 'thoma',\n",
       " 'tho',\n",
       " 'thinner',\n",
       " 'think',\n",
       " 'thing',\n",
       " 'thigh',\n",
       " 'thicker',\n",
       " 'thermal',\n",
       " 'therefor',\n",
       " 'therapi',\n",
       " 'theori',\n",
       " 'themselv',\n",
       " 'theme',\n",
       " 'thank',\n",
       " 'th',\n",
       " 'textur',\n",
       " 'test',\n",
       " 'terrifi',\n",
       " 'terrif',\n",
       " 'terribl',\n",
       " 'terri',\n",
       " 'term',\n",
       " 'tent',\n",
       " 'tend',\n",
       " 'tempt',\n",
       " 'temperatur',\n",
       " 'tell',\n",
       " 'teeni',\n",
       " 'teenag',\n",
       " 'teen',\n",
       " 'tee',\n",
       " 'technic',\n",
       " 'tear',\n",
       " 'teal',\n",
       " 'taup',\n",
       " 'tattoo',\n",
       " 'tast',\n",
       " 'target',\n",
       " 'taper',\n",
       " 'tape',\n",
       " 'tank',\n",
       " 'tan',\n",
       " 'taller',\n",
       " 'tall',\n",
       " 'talk',\n",
       " 'taken',\n",
       " 'tailor',\n",
       " 'tail',\n",
       " 'tag',\n",
       " 'tad',\n",
       " 'tabl',\n",
       " 'tab',\n",
       " 'sz',\n",
       " 'synthet',\n",
       " 'swimsuit',\n",
       " 'swim',\n",
       " 'sweatshirt',\n",
       " 'sweatpant',\n",
       " 'sweater',\n",
       " 'sweat',\n",
       " 'swap',\n",
       " 'suspect',\n",
       " 'surpris',\n",
       " 'surgeri',\n",
       " 'sure',\n",
       " 'suppos',\n",
       " 'support',\n",
       " 'suppli',\n",
       " 'suppl',\n",
       " 'superior',\n",
       " 'super',\n",
       " 'sunlight',\n",
       " 'sunglass',\n",
       " 'sun',\n",
       " 'summer',\n",
       " 'suitcas',\n",
       " 'suitabl',\n",
       " 'suit',\n",
       " 'suggest',\n",
       " 'sudden',\n",
       " 'suck',\n",
       " 'substanti',\n",
       " 'stylish',\n",
       " 'style',\n",
       " 'sturdier',\n",
       " 'sturdi',\n",
       " 'stun',\n",
       " 'stuffer',\n",
       " 'stuff',\n",
       " 'stuf',\n",
       " 'stuck',\n",
       " 'struggl',\n",
       " 'strong',\n",
       " 'stripe',\n",
       " 'strip',\n",
       " 'string',\n",
       " 'stretchi',\n",
       " 'stretch',\n",
       " 'stress',\n",
       " 'strenuous',\n",
       " 'strength',\n",
       " 'street',\n",
       " 'strapless',\n",
       " 'strap',\n",
       " 'stranger',\n",
       " 'strang',\n",
       " 'straight',\n",
       " 'stori',\n",
       " 'store',\n",
       " 'storag',\n",
       " 'stop',\n",
       " 'stood',\n",
       " 'stomach',\n",
       " 'stock',\n",
       " 'stitch',\n",
       " 'stinki',\n",
       " 'stiffer',\n",
       " 'stiff',\n",
       " 'sticker',\n",
       " 'stick',\n",
       " 'stay',\n",
       " 'static',\n",
       " 'statement',\n",
       " 'state',\n",
       " 'start',\n",
       " 'star',\n",
       " 'standard',\n",
       " 'stand',\n",
       " 'stain',\n",
       " 'stage',\n",
       " 'staff',\n",
       " 'squeez',\n",
       " 'squar',\n",
       " 'spring',\n",
       " 'spread',\n",
       " 'spot',\n",
       " 'sportswear',\n",
       " 'sport',\n",
       " 'split',\n",
       " 'splendid',\n",
       " 'splash',\n",
       " 'spit',\n",
       " 'spin',\n",
       " 'spill',\n",
       " 'spiderman',\n",
       " 'spice',\n",
       " 'spent',\n",
       " 'spend',\n",
       " 'spectrum',\n",
       " 'spectacular',\n",
       " 'specifi',\n",
       " 'specif',\n",
       " 'special',\n",
       " 'sparkl',\n",
       " 'spark',\n",
       " 'spare',\n",
       " 'spanx',\n",
       " 'spandex',\n",
       " 'space',\n",
       " 'sound',\n",
       " 'sort',\n",
       " 'sorri',\n",
       " 'sore',\n",
       " 'soooooo',\n",
       " 'sooooo',\n",
       " 'sooo',\n",
       " 'sooner',\n",
       " 'soon',\n",
       " 'soo',\n",
       " 'son',\n",
       " 'somewher',\n",
       " 'somewhat',\n",
       " 'sometim',\n",
       " 'someth',\n",
       " 'someon',\n",
       " 'somebodi',\n",
       " 'solid',\n",
       " 'sole',\n",
       " 'sold',\n",
       " 'softest',\n",
       " 'softer',\n",
       " 'soften',\n",
       " 'soft',\n",
       " 'sock',\n",
       " 'soak',\n",
       " 'snugger',\n",
       " 'snug',\n",
       " 'snow',\n",
       " 'sneak',\n",
       " 'snap',\n",
       " 'snag',\n",
       " 'smooth',\n",
       " 'smoke',\n",
       " 'smile',\n",
       " 'smell',\n",
       " 'smash',\n",
       " 'smallest',\n",
       " 'smaller',\n",
       " 'small',\n",
       " 'slowli',\n",
       " 'slouchi',\n",
       " 'slouch',\n",
       " 'slot',\n",
       " 'sloppi',\n",
       " 'slit',\n",
       " 'slipperi',\n",
       " 'slip',\n",
       " 'slimmer',\n",
       " 'slime',\n",
       " 'slim',\n",
       " 'slight',\n",
       " 'slide',\n",
       " 'slender',\n",
       " 'sleeveless',\n",
       " 'sleev',\n",
       " 'sleep',\n",
       " 'slant',\n",
       " 'slack',\n",
       " 'sky',\n",
       " 'skull',\n",
       " 'skirt',\n",
       " 'skinni',\n",
       " 'skin',\n",
       " 'skeptic',\n",
       " 'size',\n",
       " 'situat',\n",
       " 'site',\n",
       " 'sit',\n",
       " 'sister',\n",
       " 'singl',\n",
       " 'sinc',\n",
       " 'simplic',\n",
       " 'simpli',\n",
       " 'simpl',\n",
       " 'similar',\n",
       " 'silver',\n",
       " 'silli',\n",
       " 'silki',\n",
       " 'silk',\n",
       " 'silicon',\n",
       " 'signific',\n",
       " 'sign',\n",
       " 'sight',\n",
       " 'sigh',\n",
       " 'shrunk',\n",
       " 'shrug',\n",
       " 'shrinkag',\n",
       " 'shrink',\n",
       " 'shrank',\n",
       " 'shown',\n",
       " 'shower',\n",
       " 'shoulder',\n",
       " 'shot',\n",
       " 'shorter',\n",
       " 'short',\n",
       " 'shop',\n",
       " 'shoot',\n",
       " 'shoe',\n",
       " 'shock',\n",
       " 'shirt',\n",
       " 'shipment',\n",
       " 'shipe',\n",
       " 'ship',\n",
       " 'shini',\n",
       " 'shine',\n",
       " 'shin',\n",
       " 'shift',\n",
       " 'sherpa',\n",
       " 'shell',\n",
       " 'shelf',\n",
       " 'sheet',\n",
       " 'sheer',\n",
       " 'shed',\n",
       " 'shawl',\n",
       " 'sharp',\n",
       " 'shark',\n",
       " 'share',\n",
       " 'shaper',\n",
       " 'shape',\n",
       " 'shame',\n",
       " 'shade',\n",
       " 'sexi',\n",
       " 'sewn',\n",
       " 'sew',\n",
       " 'sever',\n",
       " 'seven',\n",
       " 'settl',\n",
       " 'set',\n",
       " 'servic',\n",
       " 'serv',\n",
       " 'sequin',\n",
       " 'separ',\n",
       " 'sent',\n",
       " 'sensit',\n",
       " 'sens',\n",
       " 'send',\n",
       " 'semi',\n",
       " 'seller',\n",
       " 'sell',\n",
       " 'self',\n",
       " 'select',\n",
       " 'seen',\n",
       " 'secur',\n",
       " 'section',\n",
       " 'secret',\n",
       " 'second',\n",
       " 'seat',\n",
       " 'season',\n",
       " 'search',\n",
       " 'seamstress',\n",
       " 'seam',\n",
       " 'seal',\n",
       " 'scrub',\n",
       " 'screw',\n",
       " 'screen',\n",
       " 'scratchi',\n",
       " 'scratch',\n",
       " 'scoop',\n",
       " 'school',\n",
       " 'schedul',\n",
       " 'scarv',\n",
       " 'scarf',\n",
       " 'scala',\n",
       " 'say',\n",
       " 'saw',\n",
       " 'save',\n",
       " 'savan',\n",
       " 'satisfi',\n",
       " 'satisfactori',\n",
       " 'satin',\n",
       " 'sat',\n",
       " 'sandal',\n",
       " 'sand',\n",
       " 'sampl',\n",
       " 'sale',\n",
       " 'sakka',\n",
       " 'sail',\n",
       " 'said',\n",
       " 'sag',\n",
       " 'safeti',\n",
       " 'safe',\n",
       " 'sad',\n",
       " 'run',\n",
       " 'rule',\n",
       " 'ruin',\n",
       " 'rug',\n",
       " 'ruffl',\n",
       " 'rubber',\n",
       " 'rub',\n",
       " 'royal',\n",
       " 'rout',\n",
       " 'round',\n",
       " 'rough',\n",
       " 'roomier',\n",
       " 'roomi',\n",
       " 'room',\n",
       " 'roll',\n",
       " 'rode',\n",
       " 'rock',\n",
       " 'robe',\n",
       " 'road',\n",
       " 'risk',\n",
       " 'rise',\n",
       " 'rip',\n",
       " 'ring',\n",
       " 'right',\n",
       " 'ridicul',\n",
       " 'ride',\n",
       " 'rid',\n",
       " 'rich',\n",
       " 'ribcag',\n",
       " 'rib',\n",
       " 'rhineston',\n",
       " 'review',\n",
       " 'revers',\n",
       " 'reveal',\n",
       " 'reunion',\n",
       " 'return',\n",
       " 'retro',\n",
       " 'retain',\n",
       " 'retail',\n",
       " 'result',\n",
       " 'restrict',\n",
       " 'restock',\n",
       " 'restaur',\n",
       " 'rest',\n",
       " 'respons',\n",
       " 'respond',\n",
       " 'respect',\n",
       " 'resist',\n",
       " 'research',\n",
       " 'requir',\n",
       " 'request',\n",
       " 'report',\n",
       " 'replac',\n",
       " 'repeat',\n",
       " 'repair',\n",
       " 'reorder',\n",
       " 'remov',\n",
       " 'reminisc',\n",
       " 'remind',\n",
       " 'rememb',\n",
       " 'remark',\n",
       " 'remain',\n",
       " 'reliabl',\n",
       " 'relax',\n",
       " 'relat',\n",
       " 'reinforc',\n",
       " 'regular',\n",
       " 'regret',\n",
       " 'region',\n",
       " 'regardless',\n",
       " 'regard',\n",
       " 'refund',\n",
       " 'reflect',\n",
       " 'refer',\n",
       " 'reduc',\n",
       " 'red',\n",
       " 'reconsid',\n",
       " 'recommend',\n",
       " 'reciev',\n",
       " 'recent',\n",
       " 'receiv',\n",
       " 'reccommend',\n",
       " 'reason',\n",
       " 'rear',\n",
       " 'realli',\n",
       " 'realiz',\n",
       " 'realiti',\n",
       " 'real',\n",
       " 'readi',\n",
       " 'read',\n",
       " 'reach',\n",
       " 'rayon',\n",
       " 'raw',\n",
       " 'rave',\n",
       " 'rate',\n",
       " 'rash',\n",
       " 'rare',\n",
       " 'rapid',\n",
       " 'rang',\n",
       " 'ran',\n",
       " 'rais',\n",
       " 'raini',\n",
       " 'rain',\n",
       " 'radius',\n",
       " 'race',\n",
       " 'quit',\n",
       " 'quicker',\n",
       " 'quick',\n",
       " 'question',\n",
       " 'quarter',\n",
       " 'qualiti',\n",
       " 'push',\n",
       " 'purs',\n",
       " 'purpos',\n",
       " 'purpl',\n",
       " 'purchas',\n",
       " 'punch',\n",
       " 'pullov',\n",
       " 'pull',\n",
       " 'puffi',\n",
       " 'puff',\n",
       " 'public',\n",
       " 'provid',\n",
       " 'proud',\n",
       " 'protect',\n",
       " 'pros',\n",
       " 'proport',\n",
       " 'proper',\n",
       " 'proof',\n",
       " 'prompt',\n",
       " 'promis',\n",
       " 'prom',\n",
       " 'project',\n",
       " 'prohibit',\n",
       " 'profession',\n",
       " 'producto',\n",
       " 'product',\n",
       " 'produc',\n",
       " 'process',\n",
       " 'problem',\n",
       " 'probabl',\n",
       " 'prob',\n",
       " 'prior',\n",
       " 'print',\n",
       " 'princess',\n",
       " 'prime',\n",
       " 'primari',\n",
       " 'pricey',\n",
       " 'price',\n",
       " 'previous',\n",
       " 'prevent',\n",
       " 'prettier',\n",
       " 'pretti',\n",
       " 'pressur',\n",
       " 'press',\n",
       " 'present',\n",
       " 'prepar',\n",
       " 'pregnant',\n",
       " 'pregnanc',\n",
       " 'prefer',\n",
       " 'prefect',\n",
       " 'precious',\n",
       " 'preatti',\n",
       " 'pre',\n",
       " 'practic',\n",
       " 'powder',\n",
       " 'pound',\n",
       " 'pouch',\n",
       " 'potti',\n",
       " 'potter',\n",
       " 'post',\n",
       " 'possibl',\n",
       " 'posit',\n",
       " 'portion',\n",
       " 'pop',\n",
       " 'poor',\n",
       " 'pool',\n",
       " 'polyest',\n",
       " 'polo',\n",
       " 'polit',\n",
       " 'poli',\n",
       " 'polar',\n",
       " 'pokemon',\n",
       " 'poke',\n",
       " 'pointi',\n",
       " 'point',\n",
       " 'pocket',\n",
       " 'plus',\n",
       " 'plenti',\n",
       " 'pleat',\n",
       " 'pleasant',\n",
       " 'pleas',\n",
       " 'playtex',\n",
       " 'play',\n",
       " 'plastic',\n",
       " 'plane',\n",
       " 'plan',\n",
       " 'plain',\n",
       " 'plaid',\n",
       " 'place',\n",
       " 'pjs',\n",
       " 'pj',\n",
       " 'pirat',\n",
       " 'pipe',\n",
       " 'pinkish',\n",
       " 'pink',\n",
       " 'pinch',\n",
       " 'pin',\n",
       " 'pill',\n",
       " 'piec',\n",
       " 'pictur',\n",
       " 'picki',\n",
       " 'pick',\n",
       " 'pic',\n",
       " 'phrase',\n",
       " 'photoshoot',\n",
       " 'photograph',\n",
       " 'photo',\n",
       " 'phone',\n",
       " 'pettit',\n",
       " 'petit',\n",
       " 'person',\n",
       " 'period',\n",
       " 'perhap',\n",
       " 'perform',\n",
       " 'perfect',\n",
       " 'percent',\n",
       " 'peopl',\n",
       " 'penni',\n",
       " 'pen',\n",
       " 'peel',\n",
       " 'pear',\n",
       " 'peal',\n",
       " 'peacock',\n",
       " 'pay',\n",
       " 'pattyboutik',\n",
       " 'patti',\n",
       " 'pattern',\n",
       " 'patriot',\n",
       " 'patrick',\n",
       " 'patch',\n",
       " 'past',\n",
       " 'pass',\n",
       " 'particular',\n",
       " 'parti',\n",
       " 'park',\n",
       " 'parent',\n",
       " 'paper',\n",
       " 'pantyhos',\n",
       " 'panti',\n",
       " 'pant',\n",
       " 'panel',\n",
       " 'panda',\n",
       " 'pale',\n",
       " 'pajama',\n",
       " 'pair',\n",
       " 'paint',\n",
       " 'pain',\n",
       " 'paid',\n",
       " 'page',\n",
       " 'pad',\n",
       " 'packag',\n",
       " 'pack',\n",
       " 'overweight',\n",
       " 'overs',\n",
       " 'overpow',\n",
       " 'overnight',\n",
       " 'overheat',\n",
       " 'overal',\n",
       " 'outstand',\n",
       " 'outsourc',\n",
       " 'outsid',\n",
       " 'outfit',\n",
       " 'outer',\n",
       " 'outdoor',\n",
       " 'otherwis',\n",
       " 'origin',\n",
       " 'order',\n",
       " 'orang',\n",
       " 'option',\n",
       " 'opt',\n",
       " 'opposit',\n",
       " 'oppos',\n",
       " 'opportun',\n",
       " 'opinion',\n",
       " 'open',\n",
       " 'onlin',\n",
       " 'onli',\n",
       " 'onesi',\n",
       " 'onc',\n",
       " 'oliv',\n",
       " 'older',\n",
       " 'old',\n",
       " 'okay',\n",
       " 'ok',\n",
       " 'oh',\n",
       " 'offici',\n",
       " 'offic',\n",
       " 'offer',\n",
       " 'odor',\n",
       " 'odd',\n",
       " 'octob',\n",
       " 'occasion',\n",
       " 'occas',\n",
       " 'obvious',\n",
       " 'obsess',\n",
       " 'nylon',\n",
       " 'nurs',\n",
       " 'numer',\n",
       " 'number',\n",
       " 'notic',\n",
       " 'noth',\n",
       " 'note',\n",
       " 'notch',\n",
       " 'nose',\n",
       " 'normal',\n",
       " 'nordstrom',\n",
       " 'nonetheless',\n",
       " 'non',\n",
       " 'nobodi',\n",
       " 'nippl',\n",
       " 'nip',\n",
       " 'ninja',\n",
       " 'nightgown',\n",
       " 'night',\n",
       " 'niec',\n",
       " 'nicer',\n",
       " 'nice',\n",
       " 'newborn',\n",
       " 'new',\n",
       " 'neutral',\n",
       " 'net',\n",
       " 'nervous',\n",
       " 'nephew',\n",
       " 'neopren',\n",
       " 'neic',\n",
       " 'negat',\n",
       " 'needless',\n",
       " 'need',\n",
       " 'necklin',\n",
       " 'necklac',\n",
       " 'neck',\n",
       " 'necessari',\n",
       " 'near',\n",
       " 'navi',\n",
       " 'natur',\n",
       " 'nativ',\n",
       " 'narrow',\n",
       " 'nail',\n",
       " 'muy',\n",
       " 'music',\n",
       " 'muscular',\n",
       " 'muscl',\n",
       " 'multipl',\n",
       " 'multi',\n",
       " 'muffin',\n",
       " 'movi',\n",
       " 'movement',\n",
       " 'mous',\n",
       " 'mother',\n",
       " 'mos',\n",
       " 'morn',\n",
       " 'month',\n",
       " 'money',\n",
       " 'mommi',\n",
       " 'moment',\n",
       " 'mom',\n",
       " 'mold',\n",
       " 'moistur',\n",
       " 'modest',\n",
       " 'modern',\n",
       " 'moder',\n",
       " 'model',\n",
       " 'mix',\n",
       " 'mitten',\n",
       " 'mistak',\n",
       " 'miss',\n",
       " 'mislead',\n",
       " 'mis',\n",
       " 'mirror',\n",
       " 'miracl',\n",
       " 'minut',\n",
       " 'mint',\n",
       " 'minor',\n",
       " 'minimum',\n",
       " ...]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.sort(reverse=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:32:17.065974Z",
     "start_time": "2018-11-08T00:32:17.013826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41003, 6468)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.models['count_1_gram']['X_train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Running Framework\n",
    "\n",
    "1. Supervised model or pipeline is created\n",
    "2. BayesSearchCV is configured\n",
    "3. BayesSearchCV is fitted and scored and everything is logged.\n",
    "4. Data frame maintained reporting F1, precision, accuracy, and recall (rows), and models (columns)\n",
    "5. Perferably store the confusion matrix to plot heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T05:48:24.816053Z",
     "start_time": "2018-11-08T05:46:51.965015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cnt_v', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "       ...alty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_iter=5, n_jobs=4, n_points=1,\n",
       "       optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
       "       refit=True, return_train_score=False, scoring='precision',\n",
       "       search_spaces={'lda__n_components': Integer(low=5, high=20), 'lda__learning_decay': Real(low=0.5, high=1, prior='uniform', transform='identity'), 'log_reg__C': Categorical(categories=(0.001, 0.01, 0.1, 1, 10, 100), prior=None)},\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(ar.models['orig']['X_train'], ar.models['orig']['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T05:54:31.257335Z",
     "start_time": "2018-11-08T05:54:31.203663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cnt_v': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function english_corpus at 0x1a4a922a60>,\n",
      "        vocabulary=None), 'lda': LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7366759453400837,\n",
      "             learning_method='online', learning_offset=10.0,\n",
      "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
      "             n_components=17, n_jobs=-1, n_topics=None, perp_tol=0.1,\n",
      "             random_state=42, topic_word_prior=None,\n",
      "             total_samples=1000000.0, verbose=0), 'sm': SMOTE(k=None, k_neighbors=5, kind='regular', m=None, m_neighbors=10, n_jobs=1,\n",
      "   out_step=0.5, random_state=42, ratio='auto', svm_estimator=None), 'ss': StandardScaler(copy=True, with_mean=True, with_std=True), 'log_reg': LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T05:51:23.903465Z",
     "start_time": "2018-11-08T05:51:21.462890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.04144385026737968\n",
      "Precision 0.021291208791208792\n",
      "Recall 0.775\n",
      "[[2634 1425]\n",
      " [   9   31]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(ar.models['orig']['X_train'])\n",
    "\n",
    "print('F1', f1_score(ar.models['orig']['y_train'], y_pred))\n",
    "print('Precision',precision_score(ar.models['orig']['y_train'], y_pred))\n",
    "print('Recall', recall_score(ar.models['orig']['y_train'], y_pred))\n",
    "print(confusion_matrix(ar.models['orig']['y_train'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T06:05:57.976382Z",
     "start_time": "2018-11-08T06:05:57.917065Z"
    }
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    'F1': f1_score(ar.models['orig']['y_train'], y_pred),\n",
    "    'Precision': precision_score(ar.models['orig']['y_train'], y_pred),\n",
    "    'Recall': recall_score(ar.models['orig']['y_train'], y_pred),\n",
    "    'Accuracy': accuracy_score(ar.models['orig']['y_train'],y_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T06:06:27.028538Z",
     "start_time": "2018-11-08T06:06:26.963376Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-578eacc7f89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    420\u001b[0m                                          dtype=values.dtype, copy=False)\n\u001b[1;32m    421\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data=results.values(), index=results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|metric|score|\n",
    "---|---|\n",
    "|F1| 0.03751465416178194|\n",
    "|Precision| 0.01920768307322929|\n",
    "|Recall| 0.8|\n",
    "\n",
    "||Pred No| Pred Yes|\n",
    "|---|---|---|\n",
    "|Act No| 2425| 1634|\n",
    "|Act Yes|8|   32|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T05:43:15.927806Z",
     "start_time": "2018-11-08T05:43:15.740202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4099,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.models['orig']['X_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T03:52:06.755565Z",
     "start_time": "2018-11-08T03:52:06.663464Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T03:53:07.165899Z",
     "start_time": "2018-11-08T03:53:06.793450Z"
    }
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=10)\n",
    "X_train_new = km.fit_transform(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T03:54:09.404149Z",
     "start_time": "2018-11-08T03:54:09.350705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.025     , 0.30328256, 0.02500569, ..., 0.02500768, 0.02500069,\n",
       "        0.02500012],\n",
       "       [0.54997421, 0.05000069, 0.05000473, ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       [0.00909091, 0.0090925 , 0.0090936 , ..., 0.00909194, 0.00909181,\n",
       "        0.00909094],\n",
       "       ...,\n",
       "       [0.01666777, 0.01667307, 0.84998582, ..., 0.01666673, 0.01666685,\n",
       "        0.01666713],\n",
       "       [0.00588235, 0.06961397, 0.51398135, ..., 0.00588235, 0.00588249,\n",
       "        0.00588249],\n",
       "       [0.00416708, 0.00416713, 0.28236026, ..., 0.56351615, 0.00416685,\n",
       "        0.00416674]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T22:41:40.211586Z",
     "start_time": "2018-11-07T22:41:40.114147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410034, 202)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.models['count_1_gram']['X_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T22:06:24.678100Z",
     "start_time": "2018-11-07T22:06:24.586297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'he' in set(stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T22:16:33.932559Z",
     "start_time": "2018-11-07T22:16:32.104368Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T22:35:31.658434Z",
     "start_time": "2018-11-07T22:34:15.332094Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el694431180723709041868515398\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el694431180723709041868515398_data = {\"mdsDat\": {\"x\": [-0.06691514080513457, -0.20935803272519568, -0.2291932174828441, 0.18613227013873762, 0.3193341208744374], \"y\": [0.1221636633563788, -0.1745702475190249, 0.017082857110381756, 0.26829868354004405, -0.23297495648777983], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [25.39533107231366, 23.342925160035648, 20.500229312349134, 17.3122114276431, 13.449303027658452]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [65501.0, 86041.0, 74530.0, 58594.0, 41633.0, 43721.0, 36813.0, 47901.0, 34519.0, 30940.0, 23830.0, 23100.0, 25554.0, 46729.0, 27260.0, 91946.0, 39134.0, 18819.0, 16371.0, 17186.0, 19535.0, 18334.0, 19459.0, 33598.0, 44937.0, 31098.0, 18463.0, 28250.0, 19811.0, 11885.0, 19534.223133914118, 19459.09887794633, 18462.894859053704, 13693.11559155797, 13051.508580252668, 12103.184598076376, 11781.711283103366, 11476.842419466773, 11386.824532926024, 11129.655568900336, 10802.484223727182, 10587.399422162742, 10153.266421419883, 10152.017809815587, 8326.695035530114, 7855.803471405959, 7463.035975997878, 7033.205736899039, 6723.077309836934, 6484.807472025099, 6366.909520402666, 6260.640547894319, 6213.697671426476, 5759.528314136002, 5466.181325070365, 5389.940531479832, 5094.580912776639, 4936.354818606887, 4916.177738067151, 4718.94989182542, 9771.514555813861, 35207.4360711988, 13777.719392786508, 18465.65348761508, 30230.76235756164, 15744.919617432437, 14471.738720959318, 17540.235133558657, 8192.891596332078, 15697.277792374533, 20604.231520566605, 13663.361449299755, 9046.476230835618, 8711.337015385341, 8017.309095275114, 74529.50069487623, 47901.07659302578, 30940.094097583555, 27259.382202406567, 18333.397715477025, 16022.229206699145, 14706.711177995094, 13651.076997485994, 13259.509538837927, 12348.245437675385, 11793.00672846644, 9611.709602957715, 8400.69521378411, 8113.615792417474, 7605.688830605042, 7569.200213505519, 7475.786185539866, 7079.155874032753, 6743.274918937772, 6641.305598502957, 6404.144013837996, 6236.632477235668, 6010.33492833161, 5097.694597285176, 5030.114610989949, 4947.549700755215, 4782.984871715313, 6889.180339052692, 16972.23060503309, 7404.12222136844, 11205.180149435711, 18909.196816316165, 11783.045043532162, 18412.690180783713, 25661.453463506637, 17013.80953596831, 13449.536110194727, 10733.732696989113, 6894.29852323275, 34518.2343137246, 25554.1471304915, 17185.5685071659, 13214.069217421094, 12933.676870152249, 12129.679409433456, 11208.239225851332, 9703.941484807194, 8637.360542601102, 8410.579578807463, 7930.614554117032, 7689.273752619342, 6840.56648906702, 6580.039973289753, 6315.210946932841, 6264.874781693301, 6223.361952759195, 5678.120361109768, 5429.005166861816, 5255.551155025763, 5125.327318754813, 5024.372095196795, 5003.717443273221, 4813.990689570037, 7800.6953294801115, 7514.461033555475, 14039.509680843183, 11472.698253092767, 8926.161568846574, 10647.078585099689, 27399.072850191893, 18389.027496766525, 16626.037379421134, 16679.715472806096, 11056.389780814961, 10323.399041587945, 16052.738283706833, 6915.9785024888115, 7693.8903725845585, 7672.886863894504, 8722.010890617747, 8501.452428734285, 58593.426201520466, 43721.118470893955, 23829.75116392715, 23099.30076667112, 18818.531962721427, 16370.711400990169, 8342.089655997343, 8126.563664298402, 7523.640629491859, 7258.856498944634, 7174.716618021015, 7226.770296290471, 6467.380148826191, 6215.566489720131, 6155.35460640545, 5981.593652917517, 5907.056925140028, 5669.209208849293, 5519.144602985043, 5528.7455771179675, 5143.9139555218235, 4969.364838938129, 4788.410697397688, 7627.695624051469, 4817.3397714223565, 7970.18838999495, 2709.483062557436, 10508.787984028057, 5647.492768062138, 18674.961000339852, 34118.51856874198, 8306.969012310585, 9784.57467979597, 19674.7374691949, 11521.923295565766, 65500.25192423001, 41632.46675369792, 36812.256783705525, 11884.973774468282, 10189.486690830834, 8804.342537078794, 8683.0644848565, 8516.59282728401, 8207.67412354454, 7227.001597990171, 6475.15535799048, 5932.974409610325, 5496.659091840623, 4919.163103558393, 5760.203563188763, 51922.08720484239, 11099.106121724484, 2194.877360049608, 10654.068167944062, 13164.869038028557, 1403.023794614926, 26005.689214984362, 3110.366744315736, 1486.9954562191072, 4815.2178698355465, 1903.416448999559, 6211.247650566934, 499.05043299828696, 2041.232599916678, 142.46352332169175], \"Term\": [\"love\", \"great\", \"size\", \"good\", \"shirt\", \"quality\", \"perfect\", \"small\", \"dress\", \"ordered\", \"price\", \"look\", \"cute\", \"nice\", \"large\", \"fit\", \"one\", \"product\", \"bra\", \"still\", \"long\", \"way\", \"pants\", \"comfortable\", \"well\", \"really\", \"soft\", \"material\", \"time\", \"happy\", \"long\", \"pants\", \"soft\", \"warm\", \"colors\", \"wearing\", \"jeans\", \"jacket\", \"super\", \"short\", \"thin\", \"pair\", \"length\", \"see\", \"style\", \"lot\", \"keep\", \"light\", \"comfy\", \"without\", \"weight\", \"side\", \"loose\", \"cotton\", \"extra\", \"new\", \"longer\", \"thick\", \"tall\", \"nicely\", \"enough\", \"nice\", \"fabric\", \"material\", \"like\", \"comfortable\", \"really\", \"well\", \"feel\", \"wear\", \"fit\", \"little\", \"bit\", \"time\", \"color\", \"size\", \"small\", \"ordered\", \"large\", \"way\", \"order\", \"tight\", \"medium\", \"waist\", \"work\", \"around\", \"smaller\", \"sizes\", \"item\", \"return\", \"wish\", \"true\", \"bigger\", \"run\", \"coat\", \"sizing\", \"money\", \"suit\", \"normally\", \"bottom\", \"found\", \"may\", \"usually\", \"big\", \"go\", \"top\", \"little\", \"bit\", \"would\", \"fit\", \"wear\", \"like\", \"one\", \"get\", \"dress\", \"cute\", \"still\", \"daughter\", \"beautiful\", \"old\", \"though\", \"year\", \"gift\", \"excellent\", \"put\", \"belt\", \"worn\", \"know\", \"thing\", \"something\", \"disappointed\", \"never\", \"ever\", \"absolutely\", \"bad\", \"head\", \"hard\", \"almost\", \"find\", \"make\", \"even\", \"pretty\", \"picture\", \"recommend\", \"one\", \"bought\", \"really\", \"would\", \"get\", \"got\", \"like\", \"use\", \"back\", \"much\", \"well\", \"wear\", \"good\", \"quality\", \"price\", \"look\", \"product\", \"bra\", \"cheap\", \"received\", \"since\", \"awesome\", \"costume\", \"many\", \"high\", \"shipping\", \"said\", \"fast\", \"worth\", \"brand\", \"skirt\", \"front\", \"summer\", \"thank\", \"design\", \"better\", \"husband\", \"perfectly\", \"overall\", \"made\", \"looking\", \"well\", \"great\", \"buy\", \"material\", \"fit\", \"nice\", \"love\", \"shirt\", \"perfect\", \"happy\", \"purchase\", \"day\", \"black\", \"exactly\", \"wash\", \"shorts\", \"wore\", \"white\", \"every\", \"cool\", \"came\", \"great\", \"time\", \"favorite\", \"color\", \"comfortable\", \"days\", \"fit\", \"first\", \"easy\", \"got\", \"right\", \"wear\", \"several\", \"bought\", \"perfectly\"], \"Total\": [65501.0, 86041.0, 74530.0, 58594.0, 41633.0, 43721.0, 36813.0, 47901.0, 34519.0, 30940.0, 23830.0, 23100.0, 25554.0, 46729.0, 27260.0, 91946.0, 39134.0, 18819.0, 16371.0, 17186.0, 19535.0, 18334.0, 19459.0, 33598.0, 44937.0, 31098.0, 18463.0, 28250.0, 19811.0, 11885.0, 19535.042933660752, 19459.917910891927, 18463.713533031747, 13693.932936757674, 13052.33324572083, 12104.008573412915, 11782.530188308783, 11477.663027446553, 11387.648793098171, 11130.476474336685, 10803.303629497866, 10588.218834114208, 10154.087312595404, 10152.842412519463, 8327.5192758937, 7856.62846847311, 7463.859196212104, 7034.024887769868, 6723.895895104988, 6485.63245002364, 6367.730047738547, 6261.466279657051, 6214.522463939913, 5760.348771110135, 5467.011219111441, 5390.7668794954425, 5095.403063546381, 4937.174215019545, 4917.000384400837, 4719.771750765258, 10740.507537594685, 46729.97678215364, 17853.28481274883, 28250.84740504637, 64522.632111931154, 33598.33508205387, 31098.39531164781, 44937.62137978246, 11075.111202453229, 47423.992008531706, 91946.31942931117, 38443.261929761415, 20830.18635512658, 19811.06301258438, 25667.21946612588, 74530.31546545374, 47901.88907488118, 30940.91129442222, 27260.195712742006, 18334.215765570127, 16023.046270716584, 14707.527703787433, 13651.890023001708, 13260.327837276871, 12349.071817115924, 11793.829915663267, 9612.522733900727, 8401.508380825495, 8114.4364716308055, 7606.50384925712, 7570.022994937316, 7476.604125751617, 7079.969193107836, 6744.0884845187775, 6642.123827737702, 6404.959597304397, 6237.457487612862, 6011.152971103201, 5098.507775275292, 5030.934305586344, 4948.37930017832, 4783.808451052532, 6890.61338635382, 21039.903618067616, 10116.153591174876, 17133.555554963095, 38443.261929761415, 20830.18635512658, 44556.16657268957, 91946.31942931117, 47423.992008531706, 64522.632111931154, 39134.525293611194, 23458.112385311582, 34519.04800295493, 25554.961832080768, 17186.389326782242, 13214.882029658065, 12934.490715094893, 12130.491956154818, 11209.058307353462, 9704.75324395674, 8638.173324180922, 8411.395134168046, 7931.4317492047985, 7690.087489929469, 6841.384937007735, 6580.858830854162, 6316.02888533221, 6265.69319310942, 6224.1789191003945, 5678.938039132532, 5429.819789134948, 5256.369570046618, 5126.143463703613, 5025.187372606439, 5004.536061991887, 4814.811202349568, 8962.659854741829, 8648.545362044679, 17160.535253027603, 14533.206504228114, 11386.979855809264, 13800.864739696213, 39134.525293611194, 30329.82706250329, 31098.39531164781, 44556.16657268957, 23458.112385311582, 21668.72396836111, 64522.632111931154, 9754.415974421405, 15910.727367373822, 18635.376024543748, 44937.62137978246, 47423.992008531706, 58594.2399984094, 43721.932551772225, 23830.566338534172, 23100.123053177187, 18819.346533601638, 16371.529798219042, 8342.905705200015, 8127.38712917732, 7524.472389543948, 7259.672247802614, 7175.532668494247, 7227.593857417065, 6468.20086887141, 6216.3827916003465, 6156.184630383172, 5982.407299901788, 5907.876712530252, 5670.032592243924, 5519.964617562793, 5529.571788330872, 5144.741027496365, 4970.1795519539555, 6071.9669325800805, 11897.928907576113, 8026.9757229174065, 14235.433492704013, 5902.929457781376, 23682.50128514561, 12934.667059260373, 44937.62137978246, 86041.21532555422, 21523.61954783063, 28250.84740504637, 91946.31942931117, 46729.97678215364, 65501.061771074244, 41633.277219006086, 36813.068318869875, 11885.787711266403, 10190.302572862416, 8805.158831802028, 8683.882445083222, 8517.407305602357, 8208.490209025425, 7227.814365114968, 6475.971083646071, 5933.7904448812515, 5497.474041025802, 4919.975515291024, 9169.53614205029, 86041.21532555422, 19811.06301258438, 4419.62704629349, 25667.21946612588, 33598.33508205387, 4504.442436833341, 91946.31942931117, 11089.811520313917, 5555.873665419224, 21668.72396836111, 13771.755052549945, 47423.992008531706, 4370.448725169007, 30329.82706250329, 14235.433492704013], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3706, 1.3706, 1.3706, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3705, 1.3704, 1.3704, 1.3704, 1.3704, 1.2761, 1.0875, 1.1115, 0.9454, 0.6124, 0.6126, 0.6056, 0.4298, 1.0692, 0.265, -0.1251, 0.3361, 0.5366, 0.549, 0.207, 1.4549, 1.4549, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4548, 1.4547, 1.4547, 1.4547, 1.4547, 1.4547, 1.4547, 1.4547, 1.4547, 1.24, 1.1428, 1.0302, 0.7453, 0.8851, 0.5712, 0.1787, 0.4298, -0.1132, 0.1613, 0.2304, 1.5847, 1.5847, 1.5847, 1.5847, 1.5847, 1.5847, 1.5847, 1.5847, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.5846, 1.4459, 1.4442, 1.384, 1.3483, 1.3412, 1.3253, 1.2282, 1.0844, 0.9585, 0.6022, 0.8325, 0.8433, 0.1936, 1.2408, 0.8582, 0.6974, -0.0547, -0.1342, 1.7537, 1.7537, 1.7537, 1.7537, 1.7537, 1.7537, 1.7537, 1.7537, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.7536, 1.5163, 1.3092, 1.2432, 1.1737, 0.9751, 0.9412, 0.9251, 0.8757, 0.8288, 0.8017, 0.6934, 0.2119, 0.3536, 2.0062, 2.0062, 2.0062, 2.0062, 2.0062, 2.0062, 2.0061, 2.0061, 2.0061, 2.0061, 2.0061, 2.0061, 2.0061, 2.0061, 1.5413, 1.5012, 1.4269, 1.3063, 1.127, 1.0693, 0.8398, 0.7434, 0.735, 0.6881, 0.5022, 0.0273, -0.0265, -0.1637, -0.6923, -2.5982], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.5845, -3.5884, -3.6409, -3.9398, -3.9878, -4.0632, -4.0901, -4.1163, -4.1242, -4.1471, -4.1769, -4.197, -4.2389, -4.239, -4.4372, -4.4954, -4.5467, -4.606, -4.6511, -4.6872, -4.7056, -4.7224, -4.7299, -4.8058, -4.8581, -4.8721, -4.9285, -4.96, -4.9641, -5.0051, -4.2772, -2.9954, -3.9336, -3.6408, -3.1478, -3.8002, -3.8845, -3.6922, -4.4534, -3.8032, -3.5312, -3.942, -4.3543, -4.392, -4.4751, -2.1612, -2.6033, -3.0403, -3.167, -3.5637, -3.6984, -3.7841, -3.8586, -3.8877, -3.9589, -4.0049, -4.2094, -4.3441, -4.3789, -4.4435, -4.4483, -4.4607, -4.5152, -4.5639, -4.5791, -4.6155, -4.642, -4.6789, -4.8436, -4.857, -4.8735, -4.9073, -4.5424, -3.6408, -4.4704, -4.056, -3.5328, -4.0057, -3.5594, -3.2274, -3.6384, -3.8735, -4.099, -4.5417, -2.8011, -3.1017, -3.4985, -3.7613, -3.7827, -3.8469, -3.9259, -4.07, -4.1864, -4.2131, -4.2718, -4.3027, -4.4197, -4.4585, -4.4996, -4.5076, -4.5142, -4.6059, -4.6508, -4.6833, -4.7083, -4.7282, -4.7324, -4.771, -4.2883, -4.3257, -3.7007, -3.9026, -4.1536, -3.9773, -3.032, -3.4308, -3.5316, -3.5283, -3.9395, -4.0081, -3.5667, -4.4087, -4.3021, -4.3048, -4.1767, -4.2023, -2.1029, -2.3957, -3.0026, -3.0337, -3.2387, -3.378, -4.0522, -4.0784, -4.1555, -4.1913, -4.203, -4.1957, -4.3067, -4.3465, -4.3562, -4.3848, -4.3974, -4.4385, -4.4653, -4.4636, -4.5357, -4.5702, -4.6073, -4.1417, -4.6013, -4.0978, -5.1768, -3.8213, -4.4423, -3.2463, -2.6437, -4.0564, -3.8927, -3.1942, -3.7293, -1.739, -2.1922, -2.3152, -3.4458, -3.5997, -3.7458, -3.7597, -3.779, -3.816, -3.9432, -4.0531, -4.1405, -4.2169, -4.3279, -4.1701, -1.9713, -3.5142, -5.1349, -3.5551, -3.3435, -5.5824, -2.6627, -4.7863, -5.5243, -4.3493, -5.2774, -4.0947, -6.6161, -5.2075, -7.8697]}, \"token.table\": {\"Topic\": [3, 3, 2, 4, 1, 2, 3, 3, 3, 3, 2, 4, 2, 3, 2, 1, 2, 5, 2, 1, 2, 3, 5, 4, 4, 1, 2, 3, 4, 2, 5, 4, 2, 1, 3, 4, 5, 1, 1, 4, 5, 1, 5, 4, 1, 3, 3, 5, 2, 3, 4, 5, 2, 4, 3, 3, 1, 3, 5, 1, 3, 1, 3, 3, 5, 5, 3, 1, 1, 4, 4, 1, 5, 1, 4, 3, 4, 1, 3, 5, 1, 2, 4, 5, 2, 4, 1, 2, 3, 5, 3, 2, 4, 4, 1, 2, 3, 4, 5, 4, 5, 5, 3, 3, 4, 1, 4, 2, 1, 1, 1, 3, 2, 1, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 4, 1, 3, 4, 5, 1, 1, 5, 1, 3, 4, 3, 4, 4, 1, 4, 2, 2, 2, 1, 2, 3, 4, 3, 1, 1, 4, 1, 2, 3, 1, 2, 3, 2, 2, 1, 3, 4, 1, 1, 5, 1, 4, 5, 3, 4, 3, 4, 4, 4, 5, 3, 4, 1, 3, 4, 2, 3, 2, 1, 2, 3, 4, 5, 2, 4, 1, 1, 3, 4, 5, 4, 5, 1, 5, 1, 4, 2, 2, 2, 4, 2, 2, 1, 3, 3, 1, 2, 4, 1, 1, 4, 1, 1, 3, 3, 2, 1, 5, 1, 2, 2, 1, 3, 1, 2, 2, 1, 5, 2, 1, 2, 3, 5, 1, 1, 1, 3, 4, 5, 2, 1, 5, 2, 3, 4, 1, 2, 3, 4, 3], \"Freq\": [0.9999296910078919, 0.999831519385605, 0.9999296313691819, 0.9999073997035035, 0.15417271274662567, 0.36220845640391514, 0.4835731153169743, 0.999776934900143, 0.9999620615062702, 0.9998585854932219, 0.3588859904248578, 0.6411199847683409, 0.8066576876058319, 0.19329936457064098, 0.9998631077224489, 0.4342735991785144, 0.5656694471722788, 0.9998983812725699, 0.9998142878579618, 0.23580747708390348, 0.09060387961780855, 0.6063008523624023, 0.06729349283113073, 0.9999676390523322, 0.9998178860126242, 0.21845768038925942, 0.14946370859469324, 0.24610172969415295, 0.38594809676596725, 0.37177453114195963, 0.6281669989374267, 0.9998914400771124, 0.9998308029529638, 0.31234392219930074, 0.24692974665076317, 0.025635811501451902, 0.4150819692043595, 0.9999744684942871, 0.4686244113450132, 0.13953072342873432, 0.39183489205189576, 0.9998667595217171, 0.9998017235476088, 0.9999257659997025, 0.9999394531261919, 0.999962362218066, 0.9999332548216409, 0.9998683917207896, 0.0006660091769557663, 0.5845340543081776, 0.10323142242814379, 0.3114702917563134, 0.21129891092058892, 0.7885418437161182, 0.9998105904223323, 0.9999696398650728, 0.41433627519791705, 0.3178617993047444, 0.267644674726022, 0.909826650723474, 0.09012609475034004, 0.181812510740278, 0.8181562983312509, 0.9998490209313045, 0.999913771120652, 0.9999521796260595, 0.9999530239440969, 0.9998150325523558, 0.7717347336643219, 0.22824931337509882, 0.9999319170558991, 0.5032098809932735, 0.4966482413580195, 0.7397668384751914, 0.2602231207720617, 0.8703889388229727, 0.12953743853012067, 0.22498128082968308, 0.49450795353506294, 0.2804375885291841, 0.22408727318161403, 0.2790867558296155, 0.21398355173016192, 0.28283894517380387, 0.9999233486046015, 0.9998965944646784, 0.23343736742555746, 0.293885538902811, 0.47130816914845935, 0.0013215044540161214, 0.9998641698728553, 0.7318987333742241, 0.26798723206071334, 0.9999788375374536, 0.029812553842267595, 0.2708973545729269, 0.47640091844230403, 0.0006460924981296383, 0.22220966989244345, 0.39654251594313156, 0.6034549814707135, 0.9999337266249795, 0.9998928847778801, 0.9997637157545783, 0.9998143426749175, 0.3997769659173316, 0.6001014785988739, 0.999946210481488, 0.9999422332364204, 0.9999550021684384, 0.999884885795737, 0.9998694956272066, 0.9999561370448471, 0.9998929187270179, 0.9998542956861513, 0.46853327290735025, 0.20845398831014061, 0.24879642188421466, 0.07422201858864412, 0.35540688573626444, 0.49186773054139094, 0.1526665455892656, 2.6012360809212065e-05, 0.9999466121643913, 0.9999208966314628, 0.9999513832383229, 0.3690856500695261, 0.19389753045127175, 0.4365786899754113, 0.00038655807506234404, 0.9999159285459269, 0.9999200078665255, 0.9999837900173595, 0.27716666921990807, 0.27906680634892933, 0.443745357530776, 0.8688166258543563, 0.1310046895252842, 0.9999178347000702, 0.653644109687891, 0.3463612917413632, 0.9998310026288042, 0.9999348058766802, 0.9999266547926347, 0.22983169185097574, 0.1891027041986548, 0.4117437710886147, 0.16935531624601435, 0.9998348213827888, 0.9998577420406808, 0.7534135992433382, 0.24656549806804734, 0.9998364855747245, 0.9999004070803315, 0.9999594446658391, 0.025578437261979912, 0.27428466090918324, 0.7001234790619257, 0.999934702134731, 0.9999705472662538, 0.5022929752432446, 0.03862488983320734, 0.4589246778866609, 0.9998848877102652, 0.9999528306904413, 0.9999709799014681, 0.4300536406662759, 0.559870551471777, 0.009975108947175952, 0.7838777369441158, 0.21603621251204624, 0.789433494711727, 0.21055229615775162, 0.9999762347849344, 0.9999815863106076, 0.9998721752516079, 0.9999455647834526, 0.9999786708473803, 0.46536163216690335, 0.5346256561917451, 0.999952367326526, 0.22846394479404225, 0.7714733968354481, 0.9999337607306714, 0.3410603791657124, 0.33597751211406246, 0.1086281301324049, 0.07609778100184514, 0.13818137141842682, 0.9998386016848272, 0.9998075706863427, 0.9999170269284959, 0.5312955593387513, 0.3544258490162471, 0.0002288094570795656, 0.11417591908270322, 0.99993842213178, 0.9999693221602669, 0.9999571919191614, 0.9998873289941012, 0.9999255318744483, 0.9999372195790625, 0.9999957672867509, 0.9999394893390031, 0.9998501790230182, 0.9998252493213953, 0.9999814396697426, 0.9999456194887443, 0.9999613548471454, 0.9998893668923684, 0.9999773467960699, 0.9999376433873648, 0.9998081946826601, 0.9998559640820783, 0.9999430265975041, 0.9997965457956826, 0.9997626741767324, 0.9997621686073032, 0.9998793304768083, 0.9998370993307837, 0.9999055846330314, 0.9999641201568298, 0.4397038157148155, 0.5602425267614208, 0.3459877303915958, 0.6539798446420093, 0.9999191978414992, 0.29094514806852284, 0.7090122072029259, 0.00014512496115083186, 0.9997658573680807, 0.9999752768347138, 0.999931872255985, 0.999940280244851, 0.9999336887060962, 0.33099280206474535, 0.3587635557322786, 0.1792552596261961, 0.13096746471453993, 0.9999166744300619, 0.9998853519648173, 0.39031883445195625, 0.1940912699025064, 0.41557606803821456, 0.9998667892153263, 0.999864862373866, 0.9999024844487391, 0.9998500481806469, 0.99991320666591, 0.9999437340522015, 0.9998516027715351, 0.14725234472979828, 0.4132536844245962, 0.37435895596601665, 0.06513127639168947, 0.9999223840176247], \"Term\": [\"absolutely\", \"almost\", \"around\", \"awesome\", \"back\", \"back\", \"back\", \"bad\", \"beautiful\", \"belt\", \"better\", \"better\", \"big\", \"big\", \"bigger\", \"bit\", \"bit\", \"black\", \"bottom\", \"bought\", \"bought\", \"bought\", \"bought\", \"bra\", \"brand\", \"buy\", \"buy\", \"buy\", \"buy\", \"came\", \"came\", \"cheap\", \"coat\", \"color\", \"color\", \"color\", \"color\", \"colors\", \"comfortable\", \"comfortable\", \"comfortable\", \"comfy\", \"cool\", \"costume\", \"cotton\", \"cute\", \"daughter\", \"day\", \"days\", \"days\", \"days\", \"days\", \"design\", \"design\", \"disappointed\", \"dress\", \"easy\", \"easy\", \"easy\", \"enough\", \"enough\", \"even\", \"even\", \"ever\", \"every\", \"exactly\", \"excellent\", \"extra\", \"fabric\", \"fabric\", \"fast\", \"favorite\", \"favorite\", \"feel\", \"feel\", \"find\", \"find\", \"first\", \"first\", \"first\", \"fit\", \"fit\", \"fit\", \"fit\", \"found\", \"front\", \"get\", \"get\", \"get\", \"get\", \"gift\", \"go\", \"go\", \"good\", \"got\", \"got\", \"got\", \"got\", \"got\", \"great\", \"great\", \"happy\", \"hard\", \"head\", \"high\", \"husband\", \"husband\", \"item\", \"jacket\", \"jeans\", \"keep\", \"know\", \"large\", \"length\", \"light\", \"like\", \"like\", \"like\", \"like\", \"little\", \"little\", \"little\", \"little\", \"long\", \"longer\", \"look\", \"looking\", \"looking\", \"looking\", \"looking\", \"loose\", \"lot\", \"love\", \"made\", \"made\", \"made\", \"make\", \"make\", \"many\", \"material\", \"material\", \"may\", \"medium\", \"money\", \"much\", \"much\", \"much\", \"much\", \"never\", \"new\", \"nice\", \"nice\", \"nicely\", \"normally\", \"old\", \"one\", \"one\", \"one\", \"order\", \"ordered\", \"overall\", \"overall\", \"overall\", \"pair\", \"pants\", \"perfect\", \"perfectly\", \"perfectly\", \"perfectly\", \"picture\", \"picture\", \"pretty\", \"pretty\", \"price\", \"product\", \"purchase\", \"put\", \"quality\", \"really\", \"really\", \"received\", \"recommend\", \"recommend\", \"return\", \"right\", \"right\", \"right\", \"right\", \"right\", \"run\", \"said\", \"see\", \"several\", \"several\", \"several\", \"several\", \"shipping\", \"shirt\", \"short\", \"shorts\", \"side\", \"since\", \"size\", \"sizes\", \"sizing\", \"skirt\", \"small\", \"smaller\", \"soft\", \"something\", \"still\", \"style\", \"suit\", \"summer\", \"super\", \"tall\", \"thank\", \"thick\", \"thin\", \"thing\", \"though\", \"tight\", \"time\", \"time\", \"top\", \"top\", \"true\", \"use\", \"use\", \"usually\", \"usually\", \"waist\", \"warm\", \"wash\", \"way\", \"wear\", \"wear\", \"wear\", \"wear\", \"wearing\", \"weight\", \"well\", \"well\", \"well\", \"white\", \"wish\", \"without\", \"wore\", \"work\", \"worn\", \"worth\", \"would\", \"would\", \"would\", \"would\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 5, 4, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el694431180723709041868515398\", ldavis_el694431180723709041868515398_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el694431180723709041868515398\", ldavis_el694431180723709041868515398_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el694431180723709041868515398\", ldavis_el694431180723709041868515398_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.066915  0.122164       1        1  25.395331\n",
       "1     -0.209358 -0.174570       2        1  23.342925\n",
       "4     -0.229193  0.017083       3        1  20.500229\n",
       "3      0.186132  0.268299       4        1  17.312211\n",
       "0      0.319334 -0.232975       5        1  13.449303, topic_info=     Category          Freq         Term         Total  loglift  logprob\n",
       "term                                                                    \n",
       "96    Default  65501.000000         love  65501.000000  30.0000  30.0000\n",
       "72    Default  86041.000000        great  86041.000000  29.0000  29.0000\n",
       "148   Default  74530.000000         size  74530.000000  28.0000  28.0000\n",
       "70    Default  58594.000000         good  58594.000000  27.0000  27.0000\n",
       "143   Default  41633.000000        shirt  41633.000000  26.0000  26.0000\n",
       "130   Default  43721.000000      quality  43721.000000  25.0000  25.0000\n",
       "120   Default  36813.000000      perfect  36813.000000  24.0000  24.0000\n",
       "152   Default  47901.000000        small  47901.000000  23.0000  23.0000\n",
       "44    Default  34519.000000        dress  34519.000000  22.0000  22.0000\n",
       "115   Default  30940.000000      ordered  30940.000000  21.0000  21.0000\n",
       "124   Default  23830.000000        price  23830.000000  20.0000  20.0000\n",
       "92    Default  23100.000000         look  23100.000000  19.0000  19.0000\n",
       "36    Default  25554.000000         cute  25554.000000  18.0000  18.0000\n",
       "109   Default  46729.000000         nice  46729.000000  17.0000  17.0000\n",
       "84    Default  27260.000000        large  27260.000000  16.0000  16.0000\n",
       "62    Default  91946.000000          fit  91946.000000  15.0000  15.0000\n",
       "113   Default  39134.000000          one  39134.000000  14.0000  14.0000\n",
       "127   Default  18819.000000      product  18819.000000  13.0000  13.0000\n",
       "21    Default  16371.000000          bra  16371.000000  12.0000  12.0000\n",
       "157   Default  17186.000000        still  17186.000000  11.0000  11.0000\n",
       "90    Default  19535.000000         long  19535.000000  10.0000  10.0000\n",
       "186   Default  18334.000000          way  18334.000000   9.0000   9.0000\n",
       "119   Default  19459.000000        pants  19459.000000   8.0000   8.0000\n",
       "29    Default  33598.000000  comfortable  33598.000000   7.0000   7.0000\n",
       "190   Default  44937.000000         well  44937.000000   6.0000   6.0000\n",
       "132   Default  31098.000000       really  31098.000000   5.0000   5.0000\n",
       "155   Default  18463.000000         soft  18463.000000   4.0000   4.0000\n",
       "100   Default  28250.000000     material  28250.000000   3.0000   3.0000\n",
       "174   Default  19811.000000         time  19811.000000   2.0000   2.0000\n",
       "73    Default  11885.000000        happy  11885.000000   1.0000   1.0000\n",
       "...       ...           ...          ...           ...      ...      ...\n",
       "96     Topic5  65500.251924         love  65501.061771   2.0062  -1.7390\n",
       "143    Topic5  41632.466754        shirt  41633.277219   2.0062  -2.1922\n",
       "120    Topic5  36812.256784      perfect  36813.068319   2.0062  -2.3152\n",
       "73     Topic5  11884.973774        happy  11885.787711   2.0062  -3.4458\n",
       "128    Topic5  10189.486691     purchase  10190.302573   2.0062  -3.5997\n",
       "38     Topic5   8804.342537          day   8805.158832   2.0062  -3.7458\n",
       "17     Topic5   8683.064485        black   8683.882445   2.0061  -3.7597\n",
       "51     Topic5   8516.592827      exactly   8517.407306   2.0061  -3.7790\n",
       "184    Topic5   8207.674124         wash   8208.490209   2.0061  -3.8160\n",
       "145    Topic5   7227.001598       shorts   7227.814365   2.0061  -3.9432\n",
       "195    Topic5   6475.155358         wore   6475.971084   2.0061  -4.0531\n",
       "191    Topic5   5932.974410        white   5933.790445   2.0061  -4.1405\n",
       "49     Topic5   5496.659092        every   5497.474041   2.0061  -4.2169\n",
       "31     Topic5   4919.163104         cool   4919.975515   2.0061  -4.3279\n",
       "24     Topic5   5760.203563         came   9169.536142   1.5413  -4.1701\n",
       "72     Topic5  51922.087205        great  86041.215326   1.5012  -1.9713\n",
       "174    Topic5  11099.106122         time  19811.063013   1.4269  -3.5142\n",
       "57     Topic5   2194.877360     favorite   4419.627046   1.3063  -5.1349\n",
       "27     Topic5  10654.068168        color  25667.219466   1.1270  -3.5551\n",
       "29     Topic5  13164.869038  comfortable  33598.335082   1.0693  -3.3435\n",
       "39     Topic5   1403.023795         days   4504.442437   0.8398  -5.5824\n",
       "62     Topic5  26005.689215          fit  91946.319429   0.7434  -2.6627\n",
       "61     Topic5   3110.366744        first  11089.811520   0.7350  -4.7863\n",
       "45     Topic5   1486.995456         easy   5555.873665   0.6881  -5.5243\n",
       "71     Topic5   4815.217870          got  21668.723968   0.5022  -4.3493\n",
       "136    Topic5   1903.416449        right  13771.755053   0.0273  -5.2774\n",
       "187    Topic5   6211.247651         wear  47423.992009  -0.0265  -4.0947\n",
       "141    Topic5    499.050433      several   4370.448725  -0.1637  -6.6161\n",
       "20     Topic5   2041.232600       bought  30329.827063  -0.6923  -5.2075\n",
       "121    Topic5    142.463523    perfectly  14235.433493  -2.5982  -7.8697\n",
       "\n",
       "[221 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "0         3  0.999930  absolutely\n",
       "3         3  0.999832      almost\n",
       "6         2  0.999930      around\n",
       "7         4  0.999907     awesome\n",
       "8         1  0.154173        back\n",
       "8         2  0.362208        back\n",
       "8         3  0.483573        back\n",
       "9         3  0.999777         bad\n",
       "10        3  0.999962   beautiful\n",
       "11        3  0.999859        belt\n",
       "13        2  0.358886      better\n",
       "13        4  0.641120      better\n",
       "14        2  0.806658         big\n",
       "14        3  0.193299         big\n",
       "15        2  0.999863      bigger\n",
       "16        1  0.434274         bit\n",
       "16        2  0.565669         bit\n",
       "17        5  0.999898       black\n",
       "19        2  0.999814      bottom\n",
       "20        1  0.235807      bought\n",
       "20        2  0.090604      bought\n",
       "20        3  0.606301      bought\n",
       "20        5  0.067293      bought\n",
       "21        4  0.999968         bra\n",
       "22        4  0.999818       brand\n",
       "23        1  0.218458         buy\n",
       "23        2  0.149464         buy\n",
       "23        3  0.246102         buy\n",
       "23        4  0.385948         buy\n",
       "24        2  0.371775        came\n",
       "...     ...       ...         ...\n",
       "177       2  0.999919        true\n",
       "179       1  0.290945         use\n",
       "179       3  0.709012         use\n",
       "181       1  0.000145     usually\n",
       "181       2  0.999766     usually\n",
       "182       2  0.999975       waist\n",
       "183       1  0.999932        warm\n",
       "184       5  0.999940        wash\n",
       "186       2  0.999934         way\n",
       "187       1  0.330993        wear\n",
       "187       2  0.358764        wear\n",
       "187       3  0.179255        wear\n",
       "187       5  0.130967        wear\n",
       "188       1  0.999917     wearing\n",
       "189       1  0.999885      weight\n",
       "190       1  0.390319        well\n",
       "190       3  0.194091        well\n",
       "190       4  0.415576        well\n",
       "191       5  0.999867       white\n",
       "193       2  0.999865        wish\n",
       "194       1  0.999902     without\n",
       "195       5  0.999850        wore\n",
       "196       2  0.999913        work\n",
       "197       3  0.999944        worn\n",
       "198       4  0.999852       worth\n",
       "199       1  0.147252       would\n",
       "199       2  0.413254       would\n",
       "199       3  0.374359       would\n",
       "199       4  0.065131       would\n",
       "200       3  0.999922        year\n",
       "\n",
       "[256 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 5, 4, 1])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda, ar.models['count_1_gram']['X_train'], ar.models['count_1_gram']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:23:55.625130Z",
     "start_time": "2018-11-07T23:23:55.341277Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:27:53.923770Z",
     "start_time": "2018-11-07T23:27:53.667624Z"
    }
   },
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'AmazonReviews.AmazonReviews'>: it's not the same object as AmazonReviews.AmazonReviews",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-504a3422c1d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/ar.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'AmazonReviews.AmazonReviews'>: it's not the same object as AmazonReviews.AmazonReviews"
     ]
    }
   ],
   "source": [
    "pickle.dump(ar, open('../data/ar.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:19:28.994161Z",
     "start_time": "2018-11-08T00:19:28.938200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41003,)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.models['orig']['X_train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "1. (CountVectorizer, TF-IDF) -> (LDA, PCA, NMF, Word2Vec) -> K-Means -> (Logistic Regression, Random Forest, Gradient Boost)\n",
    "2. Sampling due to imbalanced classes (SMOTE, SMOTE->Tomek, SMOTE-> ENN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
