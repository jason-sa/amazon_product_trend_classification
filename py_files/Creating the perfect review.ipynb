{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to enhance a comment to encourage trending?\n",
    "\n",
    "1. Get a list of top 10 words in the trending bucket. \n",
    "2. User enters a comment.\n",
    "3. Comment is scored.\n",
    "4. Determine if any of the top 10 words are missing, and score by adding the words to the comment.\n",
    "5. Output the comment score, and what would be the comment if 'x' word is added.\n",
    "\n",
    "Could use word2vec to find similar words through cosine similarity.\n",
    "\n",
    "Process:\n",
    "1. User enters comment\n",
    "2. Tokenize comment\n",
    "3. Find the most similar word compared to the corpus of trending products. \n",
    "4. Generate additional comments with swapping out one word.\n",
    "5. Comments are transformed (count vector -> lda -> log)\n",
    "6. Comments are predicted (SMOTE -> XGB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:08:24.089967Z",
     "start_time": "2018-11-14T04:08:12.255871Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill\n",
    "\n",
    "import re\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "MODELING_PATH = '../data/modeling/'\n",
    "PATH = '../data/amazon_reviews_us_Toys_v1_00.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:08:24.104242Z",
     "start_time": "2018-11-14T04:08:24.098262Z"
    }
   },
   "outputs": [],
   "source": [
    "# save progress\n",
    "def save(obj, obj_name):\n",
    "    f = MODELING_PATH + obj_name\n",
    "    dill.dump(obj, open(f, 'wb'))\n",
    "\n",
    "def load(obj_name):\n",
    "    f = MODELING_PATH + obj_name\n",
    "    return dill.load(open(f, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:10:24.222050Z",
     "start_time": "2018-11-14T04:08:42.919311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from pickle...\n"
     ]
    }
   ],
   "source": [
    "from AmazonReviews import AmazonReviews\n",
    "\n",
    "ar = AmazonReviews()\n",
    "ar.load_data(PATH)\n",
    "ar.calc_trend_score()\n",
    "ar.create_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:54:44.438094Z",
     "start_time": "2018-11-14T04:54:44.434558Z"
    }
   },
   "outputs": [],
   "source": [
    "# enter a comment\n",
    "comment = 'This toy is amazing! So much worth the bucks!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:54:45.208607Z",
     "start_time": "2018-11-14T04:54:45.194896Z"
    }
   },
   "outputs": [],
   "source": [
    "def token_comment(comment):\n",
    "    tkpat = re.compile('\\\\b[a-z][a-z]+\\\\b')\n",
    "    comment_token = tkpat.findall(comment)\n",
    "    return [w for w in comment_token if w not in set(stopwords.words())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:54:47.524694Z",
     "start_time": "2018-11-14T04:54:47.465784Z"
    }
   },
   "outputs": [],
   "source": [
    "comment_tokenized = token_comment(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:11:09.959429Z",
     "start_time": "2018-11-14T04:10:47.459330Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T00:35:22.439337Z",
     "start_time": "2018-11-14T00:35:22.424779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62467307\n"
     ]
    }
   ],
   "source": [
    "print (nlp.vocab[u'dog'].similarity(nlp.vocab[u'dachshund']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T01:00:25.640775Z",
     "start_time": "2018-11-14T01:00:25.025126Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_related(word):\n",
    "    # replace word.vocab with the set of words in the trending review corpus\n",
    "    filtered_words = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "#     similarity = sorted(filtered_words, key=lambda w: word.similarity(w), reverse=True)\n",
    "#     return similarity[:10]\n",
    "\n",
    "get_related(nlp.vocab[u'plane'])\n",
    "# print( [w.lower_ for w in get_related(nlp.vocab[u'plane'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:14:25.462526Z",
     "start_time": "2018-11-14T04:12:36.274100Z"
    }
   },
   "outputs": [],
   "source": [
    "## need to get the corpus of all reviews which have trended\n",
    "review_corpus = ' '.join(ar.obs[ar.obs.trend == 1].review_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:34:54.942753Z",
     "start_time": "2018-11-14T04:15:29.454659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grandchild',\n",
       " 'loved',\n",
       " 'book',\n",
       " 'used',\n",
       " 'kept',\n",
       " 'busy',\n",
       " 'creating',\n",
       " 'problem',\n",
       " 'colored',\n",
       " 'pencils']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_corpus = token_comment(review_corpus) # takes a long time\n",
    "review_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:35:38.980299Z",
     "start_time": "2018-11-14T04:35:38.908536Z"
    }
   },
   "outputs": [],
   "source": [
    "review_corpus = set(review_corpus)\n",
    "save(review_corpus, 'review_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:52:54.904847Z",
     "start_time": "2018-11-14T04:52:54.342828Z"
    }
   },
   "outputs": [],
   "source": [
    "review_vocab = [nlp.vocab[w] for w in review_corpus] # critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:35:48.928896Z",
     "start_time": "2018-11-14T04:35:48.923603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6516"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:22:42.960505Z",
     "start_time": "2018-11-14T05:22:42.956269Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_similar(word, top=10):\n",
    "#     filtered_words = [w for w in review_vocab if w.is_lower == word.is_lower]\n",
    "#     similarity_scores = [word.similarity(w) for w in review_vocab]\n",
    "#     words = [w.orth_ for w in review_vocab]\n",
    "    by_similarity = sorted(review_vocab, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [w.orth_ for w in by_similarity[:top]]\n",
    "#     return pd.DataFrame(data={'word':words, 'score':similar_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:22:04.629032Z",
     "start_time": "2018-11-14T05:22:03.114201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This toy is amazing! So much worth the bucks!!',\n",
       " 'This toys is amazing! So much worth the bucks!!',\n",
       " 'This doll is amazing! So much worth the bucks!!',\n",
       " 'This dolls is amazing! So much worth the bucks!!',\n",
       " 'This teddy is amazing! So much worth the bucks!!',\n",
       " 'This plush is amazing! So much worth the bucks!!',\n",
       " 'This bunny is amazing! So much worth the bucks!!',\n",
       " 'This stuffed is amazing! So much worth the bucks!!',\n",
       " 'This playset is amazing! So much worth the bucks!!',\n",
       " 'This lego is amazing! So much worth the bucks!!',\n",
       " 'This toy is incredible! So much worth the bucks!!',\n",
       " 'This toy is awesome! So much worth the bucks!!',\n",
       " 'This toy is fantastic! So much worth the bucks!!',\n",
       " 'This toy is wonderful! So much worth the bucks!!',\n",
       " 'This toy is great! So much worth the bucks!!',\n",
       " 'This toy is fabulous! So much worth the bucks!!',\n",
       " 'This toy is phenomenal! So much worth the bucks!!',\n",
       " 'This toy is beautiful! So much worth the bucks!!',\n",
       " 'This toy is gorgeous! So much worth the bucks!!',\n",
       " 'This toy is amazing! So even worth the bucks!!',\n",
       " 'This toy is amazing! So really worth the bucks!!',\n",
       " 'This toy is amazing! So quite worth the bucks!!',\n",
       " 'This toy is amazing! So though worth the bucks!!',\n",
       " 'This toy is amazing! So far worth the bucks!!',\n",
       " 'This toy is amazing! So better worth the bucks!!',\n",
       " 'This toy is amazing! So enough worth the bucks!!',\n",
       " 'This toy is amazing! So lot worth the bucks!!',\n",
       " 'This toy is amazing! So something worth the bucks!!',\n",
       " 'This toy is amazing! So much spend the bucks!!',\n",
       " 'This toy is amazing! So much dollars the bucks!!',\n",
       " 'This toy is amazing! So much paying the bucks!!',\n",
       " 'This toy is amazing! So much money the bucks!!',\n",
       " 'This toy is amazing! So much bucks the bucks!!',\n",
       " 'This toy is amazing! So much paid the bucks!!',\n",
       " 'This toy is amazing! So much pay the bucks!!',\n",
       " 'This toy is amazing! So much much the bucks!!',\n",
       " 'This toy is amazing! So much worthwhile the bucks!!',\n",
       " 'This toy is amazing! So much worth the dollars!!',\n",
       " 'This toy is amazing! So much worth the buck!!',\n",
       " 'This toy is amazing! So much worth the worth!!',\n",
       " 'This toy is amazing! So much worth the dollar!!',\n",
       " 'This toy is amazing! So much worth the paying!!',\n",
       " 'This toy is amazing! So much worth the money!!',\n",
       " 'This toy is amazing! So much worth the spend!!',\n",
       " 'This toy is amazing! So much worth the pay!!',\n",
       " 'This toy is amazing! So much worth the cheaper!!']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most_similar(nlp.vocab[u'plane'])\n",
    "# comment_tokenized\n",
    "\n",
    "comment_list = [comment]\n",
    "for t in comment_tokenized:\n",
    "    sim_words = most_similar(nlp.vocab[t])\n",
    "    for s in sim_words:\n",
    "        new_comment = comment.replace(t, s)\n",
    "        if new_comment != comment:\n",
    "            comment_list.append(comment.replace(t, s))\n",
    "comment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:23:11.315935Z",
     "start_time": "2018-11-14T05:23:11.263221Z"
    }
   },
   "source": [
    "## Predict on the new comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:26:15.503782Z",
     "start_time": "2018-11-14T05:26:14.851095Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_transformer = load('doc_5t_transformer.pkl')\n",
    "classifier_model = load('final_model_smote_5t.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:26:42.058181Z",
     "start_time": "2018-11-14T05:26:38.771817Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbiat', 'abov', 'ag', 'ahogi', 'alguna', 'alguno', 'ame', 'anch', 'ani', 'ant', 'anum', 'aquel', 'ata', 'atar', 'aurion', 'auron', 'avai', 'avess', 'avest', 'avet', 'avev', 'avion', 'avon', 'avrebb', 'avrest', 'avret', 'ayon', 'baiknya', 'becaus', 'befor', 'begg', 'benn', 'beraw', 'berkali', 'catr', 'dag', 'dasselb', 'deir', 'depoi', 'dera', 'derselb', 'desd', 'despr', 'dett', 'dieselb', 'diss', 'diy', 'doe', 'dond', 'durant', 'dure', 'ebb', 'egi', 'egyr', 'ell', 'ello', 'emm', 'entr', 'erai', 'erav', 'ere', 'err', 'ess', 'estabai', 'estamo', 'estaremo', 'estejamo', 'estemo', 'estev', 'estiv', 'estivemo', 'estivermo', 'estivess', 'estuv', 'estuvierai', 'estuvies', 'estuviesei', 'estuvimo', 'estuvist', 'estuvistei', 'etwa', 'eur', 'euss', 'eussion', 'facciat', 'facess', 'facest', 'facev', 'farebb', 'farest', 'faret', 'fiecar', 'foart', 'fomo', 'formo', 'foss', 'fuerai', 'fues', 'fuesei', 'fuimo', 'fuist', 'fuistei', 'fuss', 'fussion', 'habremo', 'hadd', 'hajamo', 'hatt', 'havd', 'havemo', 'hayamo', 'heill', 'hemo', 'hend', 'henn', 'hogi', 'houv', 'houvemo', 'houveremo', 'houvermo', 'houvess', 'hubierai', 'hubies', 'hubiesei', 'hubimo', 'hubist', 'hubistei', 'hvilk', 'ick', 'iet', 'ikk', 'ikkj', 'illetv', 'inaint', 'inkj', 'int', 'jela', 'joill', 'joll', 'kali', 'keill', 'kenell', 'korlei', 'kund', 'kunn', 'kurangnya', 'lenn', 'manch', 'mang', 'mase', 'mata', 'meill', 'meli', 'mill', 'minull', 'musst', 'mykj', 'nagi', 'neg', 'nerd', 'nered', 'nerey', 'niill', 'ninc', 'nist', 'niy', 'noastr', 'nogl', 'noill', 'nokr', 'nosotra', 'nosotro', 'nostr', 'notr', 'nyari', 'ohn', 'olah', 'olemm', 'olett', 'olimm', 'olisimm', 'olisitt', 'olitt', 'onc', 'onli', 'oric', 'oricar', 'oricin', 'oriund', 'ourselv', 'padah', 'pale', 'panta', 'pent', 'persz', 'pest', 'poat', 'porqu', 'printr', 'quant', 'quell', 'quest', 'reed', 'samm', 'sarebb', 'sarest', 'saret', 'seamo', 'sejamo', 'sekurang', 'semis', 'sere', 'seremo', 'serion', 'seron', 'setidak', 'siat', 'sient', 'siet', 'sill', 'sinull', 'skull', 'sobr', 'soi', 'solch', 'sollt', 'somm', 'somo', 'soyon', 'starebb', 'starest', 'staret', 'stavat', 'stess', 'stest', 'stett', 'stiat', 'sug', 'szint', 'tama', 'tanda', 'tega', 'teill', 'telj', 'temo', 'tendremo', 'tene', 'tenemo', 'tengamo', 'tenhamo', 'teremo', 'themselv', 'tidaknya', 'tien', 'tivemo', 'tivermo', 'tivess', 'toat', 'tuoll', 'tuvierai', 'tuvies', 'tuviesei', 'tuvimo', 'tuvist', 'tuvistei', 'ugyani', 'unel', 'vagi', 'vagyi', 'varj', 'veri', 'vert', 'vill', 'voastr', 'vosostra', 'vosostro', 'vostr', 'votr', 'welch', 'whi', 'wollt', 'yourselv'] not in stop_words.\n",
      "  sorted(inconsistent))\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "comments_transformed = doc_transformer.transform(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:29:52.646667Z",
     "start_time": "2018-11-14T05:29:52.642199Z"
    }
   },
   "outputs": [],
   "source": [
    "comment_probs = classifier_model.predict_proba(comments_transformed)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:29:55.562941Z",
     "start_time": "2018-11-14T05:29:55.554490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07082949 This toy is amazing! So much worth the bucks!!\n",
      "0.07082949 This toys is amazing! So much worth the bucks!!\n",
      "0.07369653 This doll is amazing! So much worth the bucks!!\n",
      "0.07369653 This dolls is amazing! So much worth the bucks!!\n",
      "0.07082949 This teddy is amazing! So much worth the bucks!!\n",
      "0.07082949 This plush is amazing! So much worth the bucks!!\n",
      "0.14121 This bunny is amazing! So much worth the bucks!!\n",
      "0.07082949 This stuffed is amazing! So much worth the bucks!!\n",
      "0.12800424 This playset is amazing! So much worth the bucks!!\n",
      "0.12800424 This lego is amazing! So much worth the bucks!!\n",
      "0.07082949 This toy is incredible! So much worth the bucks!!\n",
      "0.07082949 This toy is awesome! So much worth the bucks!!\n",
      "0.07082949 This toy is fantastic! So much worth the bucks!!\n",
      "0.07082949 This toy is wonderful! So much worth the bucks!!\n",
      "0.25630432 This toy is great! So much worth the bucks!!\n",
      "0.07082949 This toy is fabulous! So much worth the bucks!!\n",
      "0.21020529 This toy is phenomenal! So much worth the bucks!!\n",
      "0.07082949 This toy is beautiful! So much worth the bucks!!\n",
      "0.07082949 This toy is gorgeous! So much worth the bucks!!\n",
      "0.07082949 This toy is amazing! So even worth the bucks!!\n",
      "0.07082949 This toy is amazing! So really worth the bucks!!\n",
      "0.07082949 This toy is amazing! So quite worth the bucks!!\n",
      "0.07082949 This toy is amazing! So though worth the bucks!!\n",
      "0.07082949 This toy is amazing! So far worth the bucks!!\n",
      "0.07082949 This toy is amazing! So better worth the bucks!!\n",
      "0.14021012 This toy is amazing! So enough worth the bucks!!\n",
      "0.07231435 This toy is amazing! So lot worth the bucks!!\n",
      "0.07082949 This toy is amazing! So something worth the bucks!!\n",
      "0.14292574 This toy is amazing! So much spend the bucks!!\n",
      "0.07082949 This toy is amazing! So much dollars the bucks!!\n",
      "0.07082949 This toy is amazing! So much paying the bucks!!\n",
      "0.13507941 This toy is amazing! So much money the bucks!!\n",
      "0.07082949 This toy is amazing! So much bucks the bucks!!\n",
      "0.07082949 This toy is amazing! So much paid the bucks!!\n",
      "0.07082949 This toy is amazing! So much pay the bucks!!\n",
      "0.07082949 This toy is amazing! So much much the bucks!!\n",
      "0.21020529 This toy is amazing! So much worthwhile the bucks!!\n",
      "0.07082949 This toy is amazing! So much worth the dollars!!\n",
      "0.07082949 This toy is amazing! So much worth the buck!!\n",
      "0.07082949 This toy is amazing! So much worth the worth!!\n",
      "0.07082949 This toy is amazing! So much worth the dollar!!\n",
      "0.07082949 This toy is amazing! So much worth the paying!!\n",
      "0.18541679 This toy is amazing! So much worth the money!!\n",
      "0.12800424 This toy is amazing! So much worth the spend!!\n",
      "0.07082949 This toy is amazing! So much worth the pay!!\n",
      "0.07082949 This toy is amazing! So much worth the cheaper!!\n"
     ]
    }
   ],
   "source": [
    "for c, p in zip(comment_list, comment_probs):\n",
    "    print(p, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:30:47.025069Z",
     "start_time": "2018-11-14T05:30:47.020239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This toy is great! So much worth the bucks!!'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_list[comment_probs.argmax()] # pretty fucking cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:33:30.647552Z",
     "start_time": "2018-11-14T05:33:30.643246Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:35:05.352857Z",
     "start_time": "2018-11-14T05:35:05.348419Z"
    }
   },
   "outputs": [],
   "source": [
    "model_pipe = Pipeline(\n",
    "    [\n",
    "        ('step1', doc_transformer),\n",
    "        ('step2', classifier_model)\n",
    "    ]\n",
    ") # need to train on whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T05:34:31.021112Z",
     "start_time": "2018-11-14T05:34:28.723941Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.07082949, 0.07082949, 0.07369653, 0.07369653, 0.07082949,\n",
       "       0.07082949, 0.14121   , 0.07082949, 0.12800424, 0.12800424,\n",
       "       0.07082949, 0.07082949, 0.07082949, 0.07082949, 0.25630432,\n",
       "       0.07082949, 0.21020529, 0.07082949, 0.07082949, 0.07082949,\n",
       "       0.07082949, 0.07082949, 0.07082949, 0.07082949, 0.07082949,\n",
       "       0.14021012, 0.07231435, 0.07082949, 0.14292574, 0.07082949,\n",
       "       0.07082949, 0.13507941, 0.07082949, 0.07082949, 0.07082949,\n",
       "       0.07082949, 0.21020529, 0.07082949, 0.07082949, 0.07082949,\n",
       "       0.07082949, 0.07082949, 0.18541679, 0.12800424, 0.07082949,\n",
       "       0.07082949], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe.predict_proba(comment_list)[:,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
